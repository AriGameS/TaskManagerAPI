name: Continuous Deployment

on:
  push:
    branches: [ main ]
  workflow_dispatch: # Allow manual triggers

env:
  DOCKER_REGISTRY: docker.io
  DOCKER_IMAGE: aribdev/taskmanager
  AWS_REGION: us-west-2
  TERRAFORM_DIR: terraform/environments/dev

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.DOCKER_REGISTRY }}/${{ env.DOCKER_IMAGE }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
          type=raw,value=v{{date 'YYYYMMDD'}}-{{sha}}

    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          BUILDPLATFORM=${{ runner.arch }}

    - name: Test pushed image
      run: |
        docker pull ${{ env.DOCKER_REGISTRY }}/${{ env.DOCKER_IMAGE }}:latest
        docker run -d -p 5125:5125 --name test-pushed ${{ env.DOCKER_REGISTRY }}/${{ env.DOCKER_IMAGE }}:latest
        sleep 10
        curl -f http://localhost:5125/health || exit 1
        docker stop test-pushed
        docker rm test-pushed

  deploy-infrastructure:
    runs-on: ubuntu-latest
    needs: build-and-push
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Import existing TaskManager resources
      run: |
        echo "🔍 Discovering existing TaskManager resources..."
        
        # Check for existing VPCs with TaskManager tags
        VPCS=$(aws ec2 describe-vpcs --region ${{ env.AWS_REGION }} --filters "Name=tag:Project,Values=taskmanager" --query 'Vpcs[*].VpcId' --output text 2>/dev/null || echo "")
        if [ ! -z "$VPCS" ] && [ "$VPCS" != "None" ]; then
          echo "Found existing TaskManager VPCs: $VPCS"
          echo "✅ VPCs will be imported by Terraform"
        else
          echo "No existing VPCs found - will create new ones"
        fi
        
        # Check for existing Load Balancers
        ALBS=$(aws elbv2 describe-load-balancers --region ${{ env.AWS_REGION }} --query 'LoadBalancers[?contains(LoadBalancerName, `taskmanager`)].LoadBalancerArn' --output text 2>/dev/null || echo "")
        if [ ! -z "$ALBS" ] && [ "$ALBS" != "None" ]; then
          echo "Found existing Load Balancers: $ALBS"
          echo "✅ Load Balancers will be imported by Terraform"
        else
          echo "No existing Load Balancers found - will create new ones"
        fi
        
        # Check for existing ECS Clusters
        ECS_CLUSTERS=$(aws ecs list-clusters --region ${{ env.AWS_REGION }} --query 'clusterArns[?contains(@, `taskmanager`)]' --output text 2>/dev/null || echo "")
        if [ ! -z "$ECS_CLUSTERS" ] && [ "$ECS_CLUSTERS" != "None" ]; then
          echo "Found existing ECS Clusters: $ECS_CLUSTERS"
          echo "✅ ECS Clusters will be imported by Terraform"
        else
          echo "No existing ECS Clusters found - will create new ones"
        fi
        
        echo "✅ Resource discovery completed"

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: Terraform Format Check
      run: terraform fmt -check -recursive
      working-directory: ${{ env.TERRAFORM_DIR }}

    - name: Terraform Init
      run: terraform init
      working-directory: ${{ env.TERRAFORM_DIR }}

    - name: Import existing resources into Terraform state
      run: |
        echo "📥 Importing existing resources into Terraform state..."
        
        # Import VPC
        EXISTING_VPC=$(aws ec2 describe-vpcs --region ${{ env.AWS_REGION }} --filters "Name=tag:Project,Values=taskmanager" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
        if [ "$EXISTING_VPC" != "None" ] && [ ! -z "$EXISTING_VPC" ]; then
          echo "Importing VPC: $EXISTING_VPC"
          terraform import module.vpc.aws_vpc.main $EXISTING_VPC 2>/dev/null || echo "VPC import failed or already in state"
        fi
        
        # Import Load Balancer
        EXISTING_ALB=$(aws elbv2 describe-load-balancers --region ${{ env.AWS_REGION }} --query 'LoadBalancers[?contains(LoadBalancerName, `taskmanager`)].LoadBalancerArn' --output text 2>/dev/null || echo "None")
        if [ "$EXISTING_ALB" != "None" ] && [ ! -z "$EXISTING_ALB" ]; then
          echo "Importing Load Balancer: $EXISTING_ALB"
          terraform import module.alb.aws_lb.main $EXISTING_ALB 2>/dev/null || echo "ALB import failed or already in state"
        fi
        
        # Import Target Group
        EXISTING_TG=$(aws elbv2 describe-target-groups --region ${{ env.AWS_REGION }} --query 'TargetGroups[?contains(TargetGroupName, `taskmanager`)].TargetGroupArn' --output text 2>/dev/null || echo "None")
        if [ "$EXISTING_TG" != "None" ] && [ ! -z "$EXISTING_TG" ]; then
          echo "Importing Target Group: $EXISTING_TG"
          terraform import module.alb.aws_lb_target_group.app $EXISTING_TG 2>/dev/null || echo "Target Group import failed or already in state"
        fi
        
        # Import ECS Cluster
        EXISTING_CLUSTER=$(aws ecs list-clusters --region ${{ env.AWS_REGION }} --query 'clusterArns[?contains(@, `taskmanager`)][0]' --output text 2>/dev/null || echo "None")
        if [ "$EXISTING_CLUSTER" != "None" ] && [ ! -z "$EXISTING_CLUSTER" ]; then
          echo "Importing ECS Cluster: $EXISTING_CLUSTER"
          terraform import module.ecs.aws_ecs_cluster.main $EXISTING_CLUSTER 2>/dev/null || echo "ECS Cluster import failed or already in state"
        fi
        
        # Import CloudWatch Log Groups
        echo "Importing CloudWatch Log Groups..."
        terraform import module.alb.aws_cloudwatch_log_group.alb[0] /aws/applicationloadbalancer/taskmanager 2>/dev/null || echo "ALB Log Group import failed or already in state"
        terraform import module.ecs.aws_cloudwatch_log_group.ecs /ecs/taskmanager 2>/dev/null || echo "ECS Log Group import failed or already in state"
        
        # Import Secrets Manager Secret
        echo "Importing Secrets Manager Secret..."
        terraform import module.rds[0].aws_secretsmanager_secret.db_password taskmanager-db-password 2>/dev/null || echo "DB Password Secret import failed or already in state"
        
        # Import RDS Subnet Group
        echo "Importing RDS Subnet Group..."
        terraform import module.rds[0].aws_db_subnet_group.main taskmanager-db-subnet-group 2>/dev/null || echo "RDS Subnet Group import failed or already in state"
        
        # Import RDS Parameter Group
        echo "Importing RDS Parameter Group..."
        terraform import module.rds[0].aws_db_parameter_group.main taskmanager-db-params 2>/dev/null || echo "RDS Parameter Group import failed or already in state"
        
        # Import IAM Roles
        echo "Importing IAM Roles..."
        terraform import module.security.aws_iam_role.ecs_task_execution_role taskmanager-ecs-task-execution-role 2>/dev/null || echo "ECS Task Execution Role import failed or already in state"
        terraform import module.security.aws_iam_role.ecs_task_role taskmanager-ecs-task-role 2>/dev/null || echo "ECS Task Role import failed or already in state"
        terraform import module.security.aws_iam_role.ecs_service_role taskmanager-ecs-service-role 2>/dev/null || echo "ECS Service Role import failed or already in state"
        
        echo "✅ Import process completed"
      working-directory: ${{ env.TERRAFORM_DIR }}

    - name: Terraform Validate
      run: terraform validate
      working-directory: ${{ env.TERRAFORM_DIR }}

    - name: Update container image in Terraform
      run: |
        # Update the terraform.tfvars with the new image tag
        NEW_TAG="latest"
        sed -i "s|container_image = \".*\"|container_image = \"${{ env.DOCKER_REGISTRY }}/${{ env.DOCKER_IMAGE }}:$NEW_TAG\"|" terraform.tfvars
        echo "Updated container_image to: ${{ env.DOCKER_REGISTRY }}/${{ env.DOCKER_IMAGE }}:$NEW_TAG"
      working-directory: ${{ env.TERRAFORM_DIR }}

    - name: Terraform Plan
      run: terraform plan -out=tfplan
      working-directory: ${{ env.TERRAFORM_DIR }}

    - name: Terraform Apply
      run: terraform apply -auto-approve tfplan
      working-directory: ${{ env.TERRAFORM_DIR }}

    - name: Get Application URL
      run: |
        ALB_URL=$(terraform output -raw alb_url)
        echo "Application deployed at: $ALB_URL"
        echo "ALB_URL=$ALB_URL" >> $GITHUB_ENV
        
        # Wait for deployment to be ready
        echo "Waiting for application to be ready..."
        for i in {1..30}; do
          if curl -f "$ALB_URL/health" 2>/dev/null; then
            echo "✅ Application is healthy and ready!"
            break
          fi
          echo "Attempt $i: Application not ready yet, waiting..."
          sleep 10
        done

    - name: Run Health Check
      run: |
        curl -f $ALB_URL/health
        echo "✅ Health check passed!"

    - name: Test Application Endpoints
      run: |
        echo "Testing main endpoint..."
        curl -f $ALB_URL/
        echo "✅ Main endpoint working!"
        
        echo "Testing API endpoints..."
        curl -f $ALB_URL/tasks
        echo "✅ API endpoints working!"

  notify-deployment:
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-infrastructure]
    if: always()
    
    steps:
    - name: Deployment Success Notification
      if: ${{ needs.deploy-infrastructure.result == 'success' }}
      run: |
        echo "🚀 Deployment successful!"
        echo "📦 Image: ${{ needs.build-and-push.outputs.image-tag }}"
        echo "🌐 Application URL: ${{ env.ALB_URL }}"
        echo "✅ Your Flask Task Manager is now live!"

    - name: Deployment Failure Notification
      if: ${{ needs.deploy-infrastructure.result == 'failure' }}
      run: |
        echo "❌ Deployment failed!"
        echo "Please check the logs and fix any issues."
        exit 1

  cleanup:
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-infrastructure]
    if: always() && github.ref != 'refs/heads/main'
    
    steps:
    - name: Cleanup Docker images (for non-main branches)
      run: |
        echo "Cleaning up Docker images for feature branch..."
        # Add cleanup logic here if needed
